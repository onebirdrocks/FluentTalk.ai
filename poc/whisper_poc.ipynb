{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e31c9d-a71a-410e-8e57-e8726eb9b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import sys\n",
    "import time\n",
    "from functools import wraps\n",
    "import mlx_whisper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75d7303-98e6-4f61-ba96-ae003505552f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "model = whisper.load_model(\"large\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee67d3b-9732-45b1-b24b-eaf865bfec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' took {elapsed_time:.4f} seconds to complete.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b72abec-7332-477d-b180-4ea7e91228b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time\n",
    "def audio_to_text(audio_file):\n",
    "    print(\"-------------------------------------\") \n",
    "    print(\" Running audio_to_text function\") \n",
    "    print(\"-------------------------------------\")\n",
    "    result = model.transcribe(audio_file)\n",
    "    print(\"The result is:\"+result[\"text\"])\n",
    "    \n",
    "\n",
    "@measure_time\n",
    "def audio_to_text_mlx(audio_file):\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\" Running audio_to_text_mlx function\") \n",
    "    print(\"-------------------------------------\")\n",
    "    result = mlx_whisper.transcribe(audio_file, path_or_hf_repo=\"mlx-community/whisper-large-v3-mlx\")\n",
    "    #text = mlx_whisper.transcribe(audio_file)[\"text\"]\n",
    "    print(\"The result is:\"+result[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24074e-f46a-455e-960e-6f2b8dcc02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time\n",
    "def audio_to_text(audio_file):\n",
    "    result = model.transcribe(audio_file)\n",
    "    print(\"result in whisper:\"+result[\"text\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbf644c-c261-4d49-96fb-194ff0779458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      " Running audio_to_text_mlx function\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|█████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 36578.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result is: Ladies and gentlemen, today, I'd like to talk about large language models, or LLMs. These models represent a significant advancement in artificial intelligence. They are capable of understanding and generating human-like text based on vast amounts of data. LLMs can assist in various fields such as customer service, content creation, and even complex problem solving. By leveraging LLMs, we can enhance efficiency, creativity, and innovation in countless industries. The future of AI, powered by LLMs, holds immense potential to transform our world in unimaginable ways. Thank you.\n",
      "Function 'audio_to_text_mlx' took 4.7434 seconds to complete.\n",
      "-------------------------------------\n",
      " Running audio_to_text function\n",
      "-------------------------------------\n",
      "The result is: Ladies and gentlemen, today, I'd like to talk about large language models, or LLMs. These models represent a significant advancement in artificial intelligence. They are capable of understanding and generating human-like text based on vast amounts of data. LLMs can assist in various fields such as customer service, content creation, and even complex problem solving. By leveraging LLMs, we can enhance efficiency, creativity, and innovation in countless industries. The future of AI, powered by LLMs, holds immense potential to transform our world in unimaginable ways. Thank you.\n",
      "Function 'audio_to_text' took 15.8928 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "def main(audio_file):\n",
    "    audio_to_text_mlx(audio_file)\n",
    "    audio_to_text(audio_file)\n",
    "   \n",
    "\n",
    "audio_file= \"./llm_speech.mp3\"\n",
    "main(audio_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
